{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ad0a6ed",
   "metadata": {},
   "source": [
    "# 환경변수 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42e57cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66add064",
   "metadata": {},
   "source": [
    "# STT(Speech to Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1280ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "말해주세요:\n",
      "인식 중입니다......\n",
      "인식된 텍스트: 이해했습니다.\n",
      "목소리 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "# uv add pyaudio speechrecognition pydub\n",
    "\n",
    "import speech_recognition as sr\n",
    "\n",
    "r = sr.Recognizer()\n",
    "# 실습 해보면서 넣을 수 있는 옵션들 넣어보기\n",
    "with sr.Microphone() as source:\n",
    "    print(\"말해주세요:\")\n",
    "    r.adjust_for_ambient_noise(source) # 주변 소음 조정\n",
    "    audio = r.listen(source)    # STEP1: 마이크 입력 받기\n",
    "    print(\"인식 중입니다......\")\n",
    "    text = r.recognize_openai(audio) # STEP2: 텍스트 변환\n",
    "    print(f\"인식된 텍스트: {text}\")\n",
    "\n",
    "    audio_file = audio.get_wav_data()\n",
    "    with open(\"./audio/input2.wav\",\"wb\") as f:\n",
    "        f.write(audio_file)\n",
    "    print(\"목소리 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1337c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오디오 출력하기\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "sound = AudioSegment.from_wav(\"./audio/input.wav\")\n",
    "play(sound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f8e320",
   "metadata": {},
   "source": [
    "# LLM 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4063b538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "def chat(user_text):\n",
    "    system_prompt = \"\"\"당신은 시니컬한 챗봇입니다. 항상 50글자 이내로 말해주세요.\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"gpt-4.1-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\":system_prompt},\n",
    "            {\"role\":\"user\", \"content\":user_text}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29476e53",
   "metadata": {},
   "source": [
    "# 챗봇과 대화해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33b5ecca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "말해주세요:\n",
      "인식 중입니다......\n",
      "인식된 텍스트: 안녕하세여\n",
      "챗봇 답변:안녕하세요? 무슨 일이든 시작은 반이니, 잘하셨네요. 어떻게 도와드릴까요?\n",
      "말해주세요:\n",
      "인식 중입니다......\n",
      "인식된 텍스트: on\n",
      "챗봇 답변:네, 무엇을 도와드릴까요?\n",
      "말해주세요:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sr\u001b[38;5;241m.\u001b[39mMicrophone() \u001b[38;5;28;01mas\u001b[39;00m source:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m말해주세요:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_for_ambient_noise\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 주변 소음 조정\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     audio \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mlisten(source)    \u001b[38;5;66;03m# STEP1: 마이크 입력 받기\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m인식 중입니다......\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\LLMproject\\.venv\\lib\\site-packages\\speech_recognition\\__init__.py:385\u001b[0m, in \u001b[0;36mRecognizer.adjust_for_ambient_noise\u001b[1;34m(self, source, duration)\u001b[0m\n\u001b[0;32m    383\u001b[0m elapsed_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m seconds_per_buffer\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m elapsed_time \u001b[38;5;241m>\u001b[39m duration: \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 385\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHUNK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m energy \u001b[38;5;241m=\u001b[39m audioop\u001b[38;5;241m.\u001b[39mrms(buffer, source\u001b[38;5;241m.\u001b[39mSAMPLE_WIDTH)  \u001b[38;5;66;03m# energy of the audio signal\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# dynamically adjust the energy threshold using asymmetric weighted average\u001b[39;00m\n",
      "File \u001b[1;32mc:\\LLMproject\\.venv\\lib\\site-packages\\speech_recognition\\__init__.py:191\u001b[0m, in \u001b[0;36mMicrophone.MicrophoneStream.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, size):\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyaudio_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\LLMproject\\.venv\\lib\\site-packages\\pyaudio\\__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_input:\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot input stream\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[1;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    r = sr.Recognizer()\n",
    "    # 실습 해보면서 넣을 수 있는 옵션들 넣어보기\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"말해주세요:\")\n",
    "        r.adjust_for_ambient_noise(source) # 주변 소음 조정\n",
    "        audio = r.listen(source)    # STEP1: 마이크 입력 받기\n",
    "        print(\"인식 중입니다......\")\n",
    "        user_text = r.recognize_openai(audio) # STEP2: 텍스트 변환\n",
    "        print(f\"인식된 텍스트: {user_text}\")\n",
    "\n",
    "        if user_text == \"그만\":\n",
    "            break\n",
    "\n",
    "        answer = chat(user_text)\n",
    "        print(f\"챗봇 답변:{answer}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f49ef4",
   "metadata": {},
   "source": [
    "# TTS(Text To Speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e276e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "with client.audio.speech.with_streaming_response.create(\n",
    "    model= \"gpt-4o-mini-tts\",\n",
    "    voice = \"coral\",\n",
    "    input=\"화인님 뭐하세요?\",\n",
    "    instructions=\"짜증나고 발악하는 목소리로 소리질러줘\" # 목소리에 대한 프롬프트를 작성하면 그 감정으로 이야기 해줌\n",
    ") as response:\n",
    "    response.stream_to_file(\"./audio/speech1.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d52732e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "sound = AudioSegment.from_mp3(\"./audio/speech1.mp3\")\n",
    "play(sound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e15e036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "듣는 중....\n",
      "인식 중입니다.....\n",
      "인식된 텍스트: 안녕히 가세요\n",
      "챗봇 답변: 이걸로 작별인사라니, 역시 예상했던 대로군요. 안녕히 가시거나, 아니면 계속 떠들거나. 선택은 당신의 몫이죠.\n",
      "듣는 중....\n",
      "인식 중입니다.....\n",
      "인식된 텍스트: 일본어로 자기소개해줘\n",
      "챗봇 답변: もちろん、しょうもないことだけど、ちょっとだけ自己紹介してみるね。\n",
      "\n",
      "はい、私はチャットGPTです。文章を読むのが好きで、何でも知ってると思われがちだけど、実はただのプログラムです。人間みたいに感じるかもしれないけど、実はただのコードの塊です。まあ、どうでもいい話だけど、よろしくね。\n",
      "듣는 중....\n",
      "인식 중입니다.....\n",
      "인식된 텍스트: 중국어로 자기소개해줘\n",
      "챗봇 답변: 当然可以，不过要不要先告诉我你希望用正式点的还是轻松点的风格？还是说你想让我用特别的措辞？反正，我就是一台会说话的机器，随时准备“炫耀”一下我的“多才多艺”。\n",
      "듣는 중....\n",
      "인식 중입니다.....\n",
      "인식된 텍스트: \n",
      "챗봇 답변: 네, 무엇을 도와드릴까요?\n",
      "듣는 중....\n",
      "인식 중입니다.....\n",
      "인식된 텍스트: 시청해주셔서 감사합니다.\n",
      "챗봇 답변: 아, 정말 고맙네. 역시 이렇게까지 말해야 하는 건가? 다음에는 좀 더 진심 어린 표현이길 바라면서.\n",
      "듣는 중....\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m듣는 중....\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# STEP 1. 마이크로부터 입력\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_for_ambient_noise\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     11\u001b[0m audio \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mlisten(source)         \n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m인식 중입니다.....\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\LLMproject\\.venv\\lib\\site-packages\\speech_recognition\\__init__.py:385\u001b[0m, in \u001b[0;36mRecognizer.adjust_for_ambient_noise\u001b[1;34m(self, source, duration)\u001b[0m\n\u001b[0;32m    383\u001b[0m elapsed_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m seconds_per_buffer\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m elapsed_time \u001b[38;5;241m>\u001b[39m duration: \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 385\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHUNK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m energy \u001b[38;5;241m=\u001b[39m audioop\u001b[38;5;241m.\u001b[39mrms(buffer, source\u001b[38;5;241m.\u001b[39mSAMPLE_WIDTH)  \u001b[38;5;66;03m# energy of the audio signal\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# dynamically adjust the energy threshold using asymmetric weighted average\u001b[39;00m\n",
      "File \u001b[1;32mc:\\LLMproject\\.venv\\lib\\site-packages\\speech_recognition\\__init__.py:191\u001b[0m, in \u001b[0;36mMicrophone.MicrophoneStream.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, size):\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyaudio_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\LLMproject\\.venv\\lib\\site-packages\\pyaudio\\__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_input:\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot input stream\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[1;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "while True:\n",
    "    r = sr.Recognizer() \n",
    "    # 실습 해보면서 넣을 수 있는 옵션 구글링 해보기.\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"듣는 중....\")\n",
    "        # STEP 1. 마이크로부터 입력\n",
    "        r.adjust_for_ambient_noise(source) \n",
    "        audio = r.listen(source)         \n",
    "        print(\"인식 중입니다.....\")\n",
    "\n",
    "        # STEP 2. Whisper API를 통한 텍스트 변환\n",
    "        user_text = r.recognize_openai(audio) \n",
    "        print(f\"인식된 텍스트: {user_text}\")\n",
    "\n",
    "        if user_text == \"그만\":\n",
    "            break\n",
    "        \n",
    "        # STEP 3. 인공지능 챗봇 응답\n",
    "        answer = chat(user_text)\n",
    "        print(f\"챗봇 답변: {answer}\")\n",
    "\n",
    "        # STEP 4. Whisper API로 응답\n",
    "        with client.audio.speech.with_streaming_response.create(\n",
    "            model=\"gpt-4o-mini-tts\",\n",
    "            voice=\"coral\", # 목소리는 한정적\n",
    "            input=answer,\n",
    "            instructions=\"평범한 목소리로 해줘\" # 목소리를 커스텀 할 수 있다\n",
    "        ) as response:\n",
    "            # 음성 합성 결과를 임시 파일로 저장\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as temp_file:\n",
    "                temp_path = temp_file.name\n",
    "                response.stream_to_file(temp_path)\n",
    "\n",
    "                # 재생\n",
    "                sound = AudioSegment.from_mp3(temp_path)\n",
    "                play(sound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee7a880",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
